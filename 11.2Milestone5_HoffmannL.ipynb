{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course: DSC550\n",
    "## Assignment: 11.2 Project Milestone 5\n",
    "## Name: Laura Hoffmann\n",
    "## Date: 5/30/2021\n",
    "\n",
    "#### Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Milestone 3/4\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "data = pd.read_csv('strokedata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to get a cleaner feel on the data set\n",
    "data = data.rename(columns = {'id': 'ID', 'gender': 'Gender', 'age': 'Age', 'hypertension': 'Hypertension',\n",
    "                             'heart_disease': 'Heart_Disease', 'ever_married': 'Married', 'work_type': 'Work',\n",
    "                             'Residence_type': 'Residence', 'avg_glucose_level': 'Glucose', 'bmi': 'BMI',\n",
    "                             'smoking_status': 'Smoking_Status', 'stroke': 'Stroke'}, inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: (5110, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Hypertension</th>\n",
       "      <th>Heart_Disease</th>\n",
       "      <th>Married</th>\n",
       "      <th>Work</th>\n",
       "      <th>Residence</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoking_Status</th>\n",
       "      <th>Stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9046</td>\n",
       "      <td>Male</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>228.69</td>\n",
       "      <td>36.6</td>\n",
       "      <td>formerly smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>51676</td>\n",
       "      <td>Female</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Rural</td>\n",
       "      <td>202.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>31112</td>\n",
       "      <td>Male</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Rural</td>\n",
       "      <td>105.92</td>\n",
       "      <td>32.5</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>60182</td>\n",
       "      <td>Female</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>171.23</td>\n",
       "      <td>34.4</td>\n",
       "      <td>smokes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1665</td>\n",
       "      <td>Female</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Rural</td>\n",
       "      <td>174.12</td>\n",
       "      <td>24.0</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     ID  Gender   Age  Hypertension  Heart_Disease Married  \\\n",
       "0           0   9046    Male  67.0             0              1     Yes   \n",
       "1           1  51676  Female  61.0             0              0     Yes   \n",
       "2           2  31112    Male  80.0             0              1     Yes   \n",
       "3           3  60182  Female  49.0             0              0     Yes   \n",
       "4           4   1665  Female  79.0             1              0     Yes   \n",
       "\n",
       "            Work Residence  Glucose   BMI   Smoking_Status  Stroke  \n",
       "0        Private     Urban   228.69  36.6  formerly smoked       1  \n",
       "1  Self-employed     Rural   202.21   NaN     never smoked       1  \n",
       "2        Private     Rural   105.92  32.5     never smoked       1  \n",
       "3        Private     Urban   171.23  34.4           smokes       1  \n",
       "4  Self-employed     Rural   174.12  24.0     never smoked       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the dimensions of the df\n",
    "print('Dimensions:',data.shape)\n",
    "# Print the first few rows of the df\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe Data:\n",
      "       Unnamed: 0        ID      Age  Hypertension  Heart_Disease  Glucose  \\\n",
      "count     5110.00   5110.00  5110.00        5110.0        5110.00  5110.00   \n",
      "mean      2554.50  36517.83    43.23           0.1           0.05   106.15   \n",
      "std       1475.27  21161.72    22.61           0.3           0.23    45.28   \n",
      "min          0.00     67.00     0.08           0.0           0.00    55.12   \n",
      "25%       1277.25  17741.25    25.00           0.0           0.00    77.24   \n",
      "50%       2554.50  36932.00    45.00           0.0           0.00    91.88   \n",
      "75%       3831.75  54682.00    61.00           0.0           0.00   114.09   \n",
      "max       5109.00  72940.00    82.00           1.0           1.00   271.74   \n",
      "\n",
      "           BMI   Stroke  \n",
      "count  4909.00  5110.00  \n",
      "mean     28.89     0.05  \n",
      "std       7.85     0.22  \n",
      "min      10.30     0.00  \n",
      "25%      23.50     0.00  \n",
      "50%      28.10     0.00  \n",
      "75%      33.10     0.00  \n",
      "max      97.60     1.00  \n",
      "**********************************************************************\n",
      "Summarized Data:\n",
      "        Gender Married     Work Residence Smoking_Status\n",
      "count     5110    5110     5110      5110           5110\n",
      "unique       3       2        5         2              4\n",
      "top     Female     Yes  Private     Urban   never smoked\n",
      "freq      2994    3353     2925      2596           1892\n"
     ]
    }
   ],
   "source": [
    "# Find what types of variables are in the df\n",
    "# Numerical data\n",
    "print(\"Describe Data:\")\n",
    "print(round(data.describe(),2))\n",
    "print(\"*\"*70)\n",
    "# Text data\n",
    "print(\"Summarized Data:\")\n",
    "print(data.describe(include=['O']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure size\n",
    "plt.rcParams['figure.figsize'] = (20, 5)\n",
    "\n",
    "# Make subplots\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 2)\n",
    "\n",
    "# X axes, colors and bins\n",
    "x_axes = ['Age', 'Glucose'] # Histograms for the columns Age and Glucose\n",
    "colors = ['powderblue', 'tomato']\n",
    "bins = [40, 10] # Respective bin sizes\n",
    "\n",
    "# Make the graphs\n",
    "axes = axes.ravel()\n",
    "for idx, ax in enumerate(axes):\n",
    "    ax.hist(data[x_axes[idx]].dropna(), bins=bins[idx], color=colors[idx])\n",
    "    ax.set_xlabel(x_axes[idx], fontsize=20)\n",
    "    ax.set_ylabel('Counts', fontsize=20)\n",
    "    ax.tick_params(axis='both', labelsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for BMI\n",
    "plt.rcParams['figure.figsize'] = (10, 5)\n",
    "\n",
    "plt.hist(data['BMI'].dropna(), bins=25, color='plum')\n",
    "plt.xlabel('BMI', fontsize=20)\n",
    "plt.ylabel('Counts', fontsize=20)\n",
    "plt.tick_params(axis='both', labelsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strokebmi = data.loc[data['Stroke'].dropna() == 1, 'BMI']\n",
    "nonstrokebmi = data.loc[data['Stroke'].dropna() == 0, 'BMI']\n",
    "\n",
    "strokeglucose = data.loc[data['Stroke'].dropna() == 1, 'Glucose']\n",
    "nonstrokeglucose = data.loc[data['Stroke'].dropna() == 0, 'Glucose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot size\n",
    "plt.rcParams['figure.figsize'] = (10, 8)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "# Add data to the scatter plot for strokes and nonstrokes\n",
    "plt.scatter(strokebmi, strokeglucose, color='tomato', label='Stroke', s=4)\n",
    "plt.scatter(nonstrokebmi, nonstrokeglucose, color='powderblue', label='No Stroke', s=4)\n",
    "\n",
    "# Labels\n",
    "plt.title('Stroke Scatterplot for BMI vs. Glucose', fontsize=25)\n",
    "plt.xlabel('BMI', fontsize=20)\n",
    "plt.ylabel('Glucose', fontsize=20)\n",
    "plt.legend(loc=1, fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure size\n",
    "plt.rcParams['figure.figsize'] = (20, 30)\n",
    "\n",
    "# Make subplots\n",
    "fig, axes = plt.subplots(nrows = 4, ncols = 2)\n",
    "\n",
    "# GENDER: Make the data read to feed into the visulizer\n",
    "Stroke1 = data.replace({'Stroke': {1: 'Stroke', 0: 'No Stroke'}})[data['Stroke']==1]['Gender'].value_counts()\n",
    "Stroke0 = data.replace({'Stroke': {1: 'Stroke', 0: 'No Stroke'}})[data['Stroke']==0]['Gender'].value_counts()\n",
    "Stroke0 = Stroke0.reindex(index=Stroke1.index)\n",
    "\n",
    "# GENDER: Make the bar plot\n",
    "p1 = axes[0, 0].bar(Stroke1.index, Stroke1.values, color='tomato')\n",
    "p2 = axes[0, 0].bar(Stroke0.index, Stroke0.values, color='powderblue', bottom=Stroke1.values)\n",
    "axes[0, 0].set_title('Gender', fontsize=25)\n",
    "axes[0, 0].set_ylabel('Counts', fontsize=20)\n",
    "axes[0, 0].tick_params(axis='both', labelsize=15)\n",
    "axes[0, 0].legend((p1[0], p2[0]), ('Stroke', 'No Stroke'), fontsize = 15)\n",
    "\n",
    "\n",
    "# MARRIED: Make the data read to feed into the visulizer\n",
    "Married1 = data.replace({'Stroke': {1: 'Stroke', 0: 'No Stroke'}})[data['Stroke']==1]['Married'].value_counts()\n",
    "Married0 = data.replace({'Stroke': {1: 'Stroke', 0: 'No Stroke'}})[data['Stroke']==0]['Married'].value_counts()\n",
    "Married0 = Married0.reindex(index = Married1.index)\n",
    "\n",
    "# MARRIED: Make the bar plot\n",
    "p3 = axes[0, 1].bar(Married1.index, Married1.values, color='tomato')\n",
    "p4 = axes[0, 1].bar(Married0.index, Married0.values, color='powderblue', bottom=Married1.values)\n",
    "axes[0, 1].set_title('Married', fontsize=25)\n",
    "axes[0, 1].set_ylabel('Counts', fontsize=20)\n",
    "axes[0, 1].tick_params(axis='both', labelsize=15)\n",
    "axes[0, 1].legend((p3[0], p4[0]), ('Stroke', 'No Stroke'), fontsize = 15)\n",
    "\n",
    "\n",
    "# WORK: Make the data read to feed into the visulizer\n",
    "Work1 = data.replace({'Stroke': {1: 'Stroke', 0: 'No Stroke'}})[data['Stroke']==1]['Work'].value_counts()\n",
    "Work0 = data.replace({'Stroke': {1: 'Stroke', 0: 'No Stroke'}})[data['Stroke']==0]['Work'].value_counts()\n",
    "Work0 = Work0.reindex(index = Work1.index)\n",
    "\n",
    "# WORK: Make the bar plot\n",
    "p5 = axes[1, 0].bar(Work1.index, Work1.values, color='tomato')\n",
    "p6 = axes[1, 0].bar(Work0.index, Work0.values, color='powderblue', bottom=Work1.values)\n",
    "axes[1, 0].set_title('Work', fontsize=25)\n",
    "axes[1, 0].set_ylabel('Counts', fontsize=20)\n",
    "axes[1, 0].tick_params(axis='both', labelsize=15)\n",
    "axes[1, 0].legend((p5[0], p6[0]), ('Stroke', 'No Stroke'), fontsize = 15)\n",
    "\n",
    "\n",
    "# RESIDENCE: Make the data read to feed into the visulizer\n",
    "Residence1 = data.replace({'Stroke': {1: 'Stroke', 0: 'No Stroke'}})[data['Stroke']==1]['Residence'].value_counts()\n",
    "Residence0 = data.replace({'Stroke': {1: 'Stroke', 0: 'No Stroke'}})[data['Stroke']==0]['Residence'].value_counts()\n",
    "Residence0 = Residence0.reindex(index = Residence1.index)\n",
    "\n",
    "# RESIDENCE: Make the bar plot\n",
    "p7 = axes[1, 1].bar(Residence1.index, Residence1.values, color='tomato')\n",
    "p8 = axes[1, 1].bar(Residence0.index, Residence0.values, color='powderblue', bottom=Residence1.values)\n",
    "axes[1, 1].set_title('Residence', fontsize=25)\n",
    "axes[1, 1].set_ylabel('Counts', fontsize=20)\n",
    "axes[1, 1].tick_params(axis='both', labelsize=15)\n",
    "axes[1, 1].legend((p7[0], p8[0]), ('Stroke', 'No Stroke'), fontsize = 15)\n",
    "\n",
    "\n",
    "# SMOKER: Make the data read to feed into the visulizer\n",
    "Smoke1 = data.replace({'Stroke': {1: 'Stroke', 0: 'No Stroke'}})[data['Stroke']==1]['Smoking_Status'].value_counts()\n",
    "Smoke0 = data.replace({'Stroke': {1: 'Stroke', 0: 'No Stroke'}})[data['Stroke']==0]['Smoking_Status'].value_counts()\n",
    "Smoke0 = Smoke0.reindex(index = Smoke1.index)\n",
    "\n",
    "# SMOKER: Make the bar plot\n",
    "p9 = axes[2, 0].bar(Smoke1.index, Smoke1.values, color='tomato')\n",
    "p10 = axes[2, 0].bar(Smoke0.index, Smoke0.values, color='powderblue', bottom=Smoke1.values)\n",
    "axes[2, 0].set_title('Smoking Status', fontsize=25)\n",
    "axes[2, 0].set_ylabel('Counts', fontsize=20)\n",
    "axes[2, 0].tick_params(axis='both', labelsize=15)\n",
    "axes[2, 0].legend((p9[0], p10[0]), ('Stroke', 'No Stroke'), fontsize = 15)\n",
    "\n",
    "\n",
    "# HYPERTENSION: Make the data read to feed into the visulizer\n",
    "# Have to replace hypertension numbers (0=no hypertension, 1=hypertension)\n",
    "Hypertension1 = data.replace({'Stroke': {1: 'Stroke', 0: 'No Stroke'}}).replace({'Hypertension': {0: 'No Hypertension', 1: 'Hypertension'}})[data['Stroke']==1]['Hypertension'].value_counts()\n",
    "Hypertension0 = data.replace({'Stroke': {1: 'Stroke', 0: 'No Stroke'}}).replace({'Hypertension': {0: 'No Hypertension', 1: 'Hypertension'}})[data['Stroke']==0]['Hypertension'].value_counts()\n",
    "Hypertension0 = Hypertension0.reindex(index = Hypertension1.index)\n",
    "\n",
    "# HYPERTENSION: Make the bar plot\n",
    "p11 = axes[2, 1].bar(Hypertension1.index, Hypertension1.values, color='tomato')\n",
    "p12 = axes[2, 1].bar(Hypertension0.index, Hypertension0.values, color='powderblue', bottom=Hypertension1.values)\n",
    "axes[2, 1].set_title('Hypertension', fontsize=25)\n",
    "axes[2, 1].set_ylabel('Counts', fontsize=20)\n",
    "axes[2, 1].tick_params(axis='both', labelsize=15)\n",
    "axes[2, 1].legend((p11[0], p12[0]), ('Stroke', 'No Stroke'), fontsize = 15)\n",
    "\n",
    "\n",
    "# HEART DISEASE: Make the data read to feed into the visulizer\n",
    "# Have to replace hypertension numbers (0=no heart disease, 1=heart disease)\n",
    "HD1 = data.replace({'Stroke': {1: 'Stroke', 0: 'No Stroke'}}).replace({'Heart_Disease': {0: 'No Heart Disease', 1: 'Heart Disease'}})[data['Stroke']==1]['Heart_Disease'].value_counts()\n",
    "HD0 = data.replace({'Stroke': {1: 'Stroke', 0: 'No Stroke'}}).replace({'Heart_Disease': {0: 'No Heart Disease', 1: 'Heart Disease'}})[data['Stroke']==0]['Heart_Disease'].value_counts()\n",
    "HD0 = HD0.reindex(index = HD1.index)\n",
    "\n",
    "# HEART DISEASE: Make the bar plot\n",
    "p11 = axes[3, 0].bar(HD1.index, HD1.values, color='tomato')\n",
    "p12 = axes[3, 0].bar(HD0.index, HD0.values, color='powderblue', bottom=HD1.values)\n",
    "axes[3, 0].set_title('Heart Disease', fontsize=25)\n",
    "axes[3, 0].set_ylabel('Counts', fontsize=20)\n",
    "axes[3, 0].tick_params(axis='both', labelsize=15)\n",
    "axes[3, 0].legend((p11[0], p12[0]), ('Stroke', 'No Stroke'), fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I liked these graphs a lot better than the originals because they seem easier to interpret. Based on these graphs it looks like males and females are almost equally likely to have strokes, where males may be slightly more likely. Married people seem to have more strokes than non married people because the bar for total married people is about doubled than the non married but the number of strokes in married people seems about 5x the number of strokes in non married people. Working as self employed or for the government might slightly increase the risk, residence area seems to not affect the risk very much, but obviously smoking is a big factor.\n",
    "\n",
    "Each of these graphs reveals just how much each feature can affect the risk of stroke. Obviously there are some things the patients can do to work on a lower stroke risk, by living a more healthy lifestyle they could lower their BMI, and quit smoking to lower their risk. However there are other factors, such as being married that would have an affect on the risk of stroke that would be more difficult to change. I believe in that case in particular, it would be more about moving toward a healthy relationship that lowers stress. Overall, most of these are still factors that could affect the risk of stroke and by using them and a model to predict the likelyhood of a stroke. Then other factors could be adjusted to prevent that happening.\n",
    "\n",
    "# Milestone 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data1= data\n",
    "# Dropping ID and Unnamed column because they contain no relevant data\n",
    "data1 = data1.drop(['Unnamed: 0', 'ID'], axis=1)\n",
    "\n",
    "ord_enc = OrdinalEncoder()\n",
    "\n",
    "# Encode categorical data into numerical for feature analysis and reduction\n",
    "data1['Gender'] = ord_enc.fit_transform(data1[['Gender']])\n",
    "data1['Married'] = ord_enc.fit_transform(data1[['Married']])\n",
    "data1['Work'] = ord_enc.fit_transform(data1[['Work']])\n",
    "data1['Residence'] = ord_enc.fit_transform(data1[['Residence']])\n",
    "data1['Smoking_Status'] = ord_enc.fit_transform(data1[['Smoking_Status']])\n",
    "\n",
    "data1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking and adjusting missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "data1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with mean for BMI\n",
    "# Define function\n",
    "def na_mean(data, inplace=True):\n",
    "    return data.fillna(round(data.mean(),2), inplace=inplace)\n",
    "\n",
    "# Fill BMI nan values\n",
    "na_mean(data1['BMI'])\n",
    "\n",
    "data1['BMI'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck\n",
    "data1['BMI'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature reduction check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Pearson Correlation\n",
    "plt.figure(figsize=(12,10))\n",
    "cor = data1.corr()\n",
    "sns.heatmap(round(cor,1), annot=True, cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the varaibles are so closely correlated with each other that I'd want to remove them now, for being almost identicle only one would suffice but none are too close. Marriage and age are the two most closely correlated with each other but I'm choosing to keep both because of how they impact the outcome of stroke differently. Obviously age is a major factor when it comes to the possibility of stroke but the stress of marriage or lack thereof can also play a role. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Correlation with output variable\n",
    "cor = data1.corr()\n",
    "cor_target = abs(cor[\"Stroke\"])\n",
    "\n",
    "# Selecting highly correlated features\n",
    "relevant_features = cor_target[cor_target>0.04]\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above I tested correlations with the target variable (Stroke) to see which features or columns might be irrelevant if they had close correlations with each other. Then I reviewed the correlations between the features and the target varaible to see which are most highly correlated with Stroke. Gender and residence were the lowest correlated features with the target variable, Stroke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns with no relevant data in original dataset\n",
    "data = data.drop(['Unnamed: 0','ID'], axis=1)\n",
    "# Replacing the missing values in data BMI with the mean\n",
    "na_mean(data['BMI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust skewed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure size\n",
    "plt.rcParams['figure.figsize'] = (20, 5)\n",
    "\n",
    "# Make subplots\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 2)\n",
    "\n",
    "# X axes, colors and bins\n",
    "x_axes = ['Glucose', 'BMI'] # Histograms for the columns Age and Glucose\n",
    "colors = ['powderblue', 'tomato']\n",
    "bins = [40, 30] # Respective bin sizes\n",
    "\n",
    "# Make the graphs\n",
    "axes = axes.ravel()\n",
    "for idx, ax in enumerate(axes):\n",
    "    ax.hist(data[x_axes[idx]].dropna(), bins=bins[idx], color=colors[idx])\n",
    "    ax.set_xlabel(x_axes[idx], fontsize=20)\n",
    "    ax.set_ylabel('Counts', fontsize=20)\n",
    "    ax.tick_params(axis='both', labelsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have two variables that are right skewed so I'll adjust them using the logarithms of both of these features for transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transformation(data):\n",
    "    return round(data.apply(np.log),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Glucose_log'] = log_transformation(data['Glucose'])\n",
    "data['BMI_log'] = log_transformation(data['BMI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Glucose log histogram\n",
    "plt.rcParams['figure.figsize'] = (10, 5)\n",
    "\n",
    "plt.hist(data['Glucose_log'], bins=40, color='powderblue')\n",
    "plt.title('Glucose Log Distribution', fontsize=25)\n",
    "plt.xlabel('Glucose_log', fontsize=20)\n",
    "plt.ylabel('Counts', fontsize=20)\n",
    "plt.tick_params(axis='both', labelsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BMI log histogram\n",
    "plt.rcParams['figure.figsize'] = (10, 5)\n",
    "\n",
    "plt.hist(data['BMI_log'], bins=40, color='tomato')\n",
    "plt.title('BMI Log Distribution', fontsize=25)\n",
    "plt.xlabel('BMI_log', fontsize=20)\n",
    "plt.ylabel('Counts', fontsize=20)\n",
    "plt.tick_params(axis='both', labelsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dummy variables for categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical features list\n",
    "data['Married'] = ord_enc.fit_transform(data[['Married']])\n",
    "cat_features = ['Gender', 'Hypertension', 'Heart_Disease', 'Married', 'Work', 'Smoking_Status']\n",
    "data_cat = data[cat_features]\n",
    "\n",
    "# Not necessary to make dummy variables for this as it is already encoded properly\n",
    "#data_cat = data_cat.replace({'Hypertension': {0: 'No', 1: 'Yes'}})\n",
    "#data_cat = data_cat.replace({'Heart_Disease': {0: 'No', 1: 'Yes'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cat_dummies = pd.get_dummies(data_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_cat_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3\n",
    "\n",
    "Below was progress made before dealing with the imbalanced data in the data set. So the work from this milestone was commented out and then later applied to the data after dealing with the imbalanced issue.\n",
    "\n",
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a whole features dataset that can be used for train and validation data splitting\n",
    "# features_model = ['Age', 'Glucose_log', 'BMI_log']\n",
    "# data_model_X = pd.concat([data[features_model], data_cat_dummies], axis=1)\n",
    "# data_model_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a whole target dataset that can be used for train and validation data splitting\n",
    "# data_model_y = data.replace({'Stroke': {1: 'Stroke', 0: 'No_Stroke'}})['Stroke']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split the data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(data_model_X, data_model_y, test_size =0.30, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # number of samples in each set\n",
    "# print(\"Number of samples in training set: \", X_train.shape[0])\n",
    "# print(\"Number of samples in validation set:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stroke and no stroke (training set)\n",
    "# print('Number of strokes and non strokes in the training set:')\n",
    "# print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Survived and not-survived (testing set)\n",
    "# print('Number of strokes and non strokes in the testing set:')\n",
    "# print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Instantiate the classification model \n",
    "# model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# #The ConfusionMatrix visualizer taxes a model\n",
    "# classes = ['No Stroke','Stroke']\n",
    "# cm = ConfusionMatrix(model, classes=classes)\n",
    "\n",
    "# #Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "# cm.fit(X_train, y_train)\n",
    "\n",
    "# #To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# #and then creates the confusion_matrix from scikit learn.\n",
    "# cm.score(X_test, y_test)\n",
    "\n",
    "# # change fontsize of the labels in the figure\n",
    "# for label in cm.ax.texts:\n",
    "#     label.set_size(20)\n",
    "\n",
    "# #How did we do?\n",
    "# cm.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # set the size of the figure and the font size \n",
    "# %matplotlib inline\n",
    "# plt.rcParams['figure.figsize'] = (15, 7)\n",
    "# plt.rcParams['font.size'] = 20\n",
    "\n",
    "# # Instantiate the visualizer\n",
    "# visualizer = ClassificationReport(model, classes=classes)\n",
    "\n",
    "# visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "# visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "# g = visualizer.poof()             # Draw/show/poof the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short conclusion\n",
    "Obviously logistic regression is not a great machine learning algorithm for the data set, it missed predicting literally any strokes. I'm believing it's primarily because of the lower correlation coefficients between the predictor variables and the target variable. Even though this algorithm is 95% accurate it's still lacking because of all of the strokes it failed to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object_type_columns = data.select_dtypes(include='object')\n",
    "# le = LabelEncoder()\n",
    "# for title in object_type_columns:\n",
    "#     data[title] = le.fit_transform(data[title])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = data.drop(['Stroke'], axis=1)\n",
    "# y = data['Stroke']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# print('absolut values for train set: ', Counter(y_train), sep='\\n')\n",
    "# print()\n",
    "# print('in percents for train set: ', y_train.value_counts(normalize=True).round(2) * 100, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DT_clf = tree.DecisionTreeClassifier()\n",
    "# DT_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DT_prediction = DT_clf.predict(X_test)\n",
    "# DT_metrics = metrics.f1_score(y_test, DT_prediction).round(2)\n",
    "# DT_report = metrics.classification_report(y_test, DT_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #The ConfusionMatrix visualizer taxes a model\n",
    "# classes = ['No Stroke','Stroke']\n",
    "# cm = ConfusionMatrix(DT_clf, classes=classes)\n",
    "\n",
    "# #Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "# cm.fit(X_train, y_train)\n",
    "\n",
    "# #To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# #and then creates the confusion_matrix from scikit learn.\n",
    "# cm.score(X_test, y_test)\n",
    "\n",
    "# # change fontsize of the labels in the figure\n",
    "# for label in cm.ax.texts:\n",
    "#     label.set_size(20)\n",
    "\n",
    "# #How did we do?\n",
    "# cm.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(DT_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While logistic regression may have a higher accuracy rate I actually like the results from the decision tree more because it caught more of the strokes than logistic regression. Even though there are more false positives I believe in this case it would be better to stick with the lower accuracy but be able to predict more of the strokes. I will try to tune this machine learning algorithm in turn to possibly increase the accuracy. After all at least this ML algorithm predicted strokes at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 4\n",
    "\n",
    "## Dealing with Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find number of positives and negatives\n",
    "y = data.Stroke\n",
    "\n",
    "df_majority = data[data.Stroke==0]\n",
    "df_minority = data[data.Stroke==1]\n",
    "\n",
    "nclass0 = len(df_majority)\n",
    "print(\"Number of cases with no stroke:\", nclass0)\n",
    "nclass1 = len(df_minority)\n",
    "print(\"Number of cases with a stroke:\", nclass1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the classes are very imbalanced so I will correct this by downsampling the cases with no stroke. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# # Resample the minority class. You can change the strategy to 'auto' if you are not sure.\n",
    "# sm = SMOTE(sampling_strategy='minority', random_state=7)\n",
    "\n",
    "# # Fit the model to generate the data.\n",
    "# oversampled_trainX, oversampled_trainY = sm.fit_resample(data.drop('Stroke', axis=1), data['Stroke'])\n",
    "# oversampled_train = pd.concat([pd.DataFrame(oversampled_trainY), pd.DataFrame(oversampled_trainX)], axis=1)\n",
    "# oversampled_train.columns = normalized_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample the number of negative cases\n",
    "from sklearn.utils import resample\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=249)     # to match minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data frame with less data points\n",
    "ds_df = pd.concat([df_majority_downsampled, df_minority])\n",
    "ds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsdata_cat = ds_df[cat_features]\n",
    "data_cat_dummies = pd.get_dummies(dsdata_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a whole features dataset that can be used for train and validation data splitting\n",
    "features_model = ['Age', 'Glucose_log', 'BMI_log']\n",
    "data_model_X = pd.concat([ds_df[features_model], data_cat_dummies], axis=1)\n",
    "data_model_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a whole target dataset that can be used for train and validation data splitting\n",
    "data_model_y = ds_df.replace({'Stroke': {1: 'Stroke', 0: 'No_Stroke'}})['Stroke']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_model_X, data_model_y, test_size =0.25, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples in each set\n",
    "print(\"Number of samples in training set: \", X_train.shape[0])\n",
    "print(\"Number of samples in validation set:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stroke and no stroke (training set)\n",
    "print('Number of strokes and non strokes in the training set:')\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Survived and not-survived (testing set)\n",
    "print('Number of strokes and non strokes in the testing set:')\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classification model \n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "#The ConfusionMatrix visualizer taxes a model\n",
    "classes = ['No Stroke','Stroke']\n",
    "cm = ConfusionMatrix(model, classes=classes)\n",
    "\n",
    "#Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "\n",
    "#To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "#and then creates the confusion_matrix from scikit learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# change fontsize of the labels in the figure\n",
    "for label in cm.ax.texts:\n",
    "    label.set_size(20)\n",
    "\n",
    "#How did we do?\n",
    "cm.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the size of the figure and the font size \n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 7)\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "# Instantiate the visualizer\n",
    "visualizer = ClassificationReport(model, classes=classes)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()             # Draw/show/poof the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_type_columns = ds_df.select_dtypes(include='object')\n",
    "le = LabelEncoder()\n",
    "for title in object_type_columns:\n",
    "    ds_df[title] = le.fit_transform(ds_df[title])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the target variable\n",
    "X = ds_df.drop(['Stroke'], axis=1)\n",
    "y = ds_df['Stroke']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into training and testing groups\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=27)\n",
    "print('absolut values for train set: ', Counter(y_train), sep='\\n')\n",
    "print()\n",
    "print('in percents for train set: ', y_train.value_counts(normalize=True).round(2) * 100, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DT_clf = tree.DecisionTreeClassifier()\n",
    "DT_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_prediction = DT_clf.predict(X_test)\n",
    "DT_metrics = metrics.f1_score(y_test, DT_prediction).round(2)\n",
    "DT_report = metrics.classification_report(y_test, DT_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The ConfusionMatrix visualizer taxes a model\n",
    "classes = ['No Stroke','Stroke']\n",
    "cm = ConfusionMatrix(DT_clf, classes=classes)\n",
    "\n",
    "#Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "\n",
    "#To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "#and then creates the confusion_matrix from scikit learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# change fontsize of the labels in the figure\n",
    "for label in cm.ax.texts:\n",
    "    label.set_size(20)\n",
    "\n",
    "#How did we do?\n",
    "cm.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(DT_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the the imbalanced issue is dealt with, working toward tuning the machine learning algorithm will be slightly easier. The logistic regression algorithm is no longer predicting 0 strokes which is a good sign. For the next milestone I will try tuning the algorithms to increase precision of the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 5\n",
    "\n",
    "## Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a whole features dataset that can be used for train and validation data splitting\n",
    "features_model = ['Age', 'Glucose_log', 'BMI_log']\n",
    "data_model_X = pd.concat([data[features_model], data_cat_dummies], axis=1)\n",
    "data_model_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a whole target dataset that can be used for train and validation data splitting\n",
    "data_model_y = data.replace({'Stroke': {1: 'Stroke', 0: 'No_Stroke'}})['Stroke']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "from sklearn.metrics import f1_score\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier( max_depth= 10, random_state=0, n_estimators=10)\n",
    "RFF = RF.fit(X_train, y_train)\n",
    "#Get the Score of Random Forest Classifier both Train and Test\n",
    "RFM_Train = RF.score(X_train, y_train)\n",
    "RFM_Test = RF.score(X_test, y_test)\n",
    "print('Random Forest Train Score: ' + str(RFM_Train))\n",
    "print('Random Forest Test Score: ' + str(RFM_Test))\n",
    "#Predict value of RFM\n",
    "RFM_pred = RF.predict(X_test)\n",
    "print(RFM_pred[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, RFM_pred, average='weighted') \n",
    "# Compute confusion matrix  \n",
    "RFM_matrix = confusion_matrix(y_test, RFM_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print (classification_report(y_test, RFM_pred))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(RFM_matrix, classes=['No Stroke','Stroke'],normalize= False,  title='Random Forest Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
