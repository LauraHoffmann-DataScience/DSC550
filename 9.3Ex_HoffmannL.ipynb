{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course: DSC550\n",
    "## Assignment: 9.3 Exercise\n",
    "## Name: Laura Hoffmann\n",
    "## Date: 5/22/2021\n",
    "\n",
    "#### Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "# Word Processing\n",
    "import re\n",
    "import sys\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import joblib\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    " \n",
    "with jsonlines.open('categorized-comments.jsonl') as reader:\n",
    "    for obj in reader.iter(type=dict, skip_invalid=True):\n",
    "        data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "606475"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of punctuation\n",
    "punctuation_dict = dict.fromkeys(i for i in range(sys.maxunicode) \n",
    "                            if unicodedata.category(chr(i)).startswith('P'))\n",
    "# Create a dictionary of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stopwords_dict = Counter(stop_words)\n",
    "\n",
    "def cleanText(string):\n",
    "    '''Processes string and returns cleaned up list of words'''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    string = string.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    string = re.sub(r'http\\S+', '', string)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    string = string.translate(punctuation_dict)\n",
    "    \n",
    "    # Remove newlines\n",
    "    string = string.replace(\"\\n\", \" \")\n",
    "    \n",
    "    # Remove stopwords\n",
    "    string = [word for word in string.split() if word not in stopwords_dict]\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.txt = data.txt.apply(lambda string: cleanText(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "data['txt_stems'] = data.txt.apply(lambda words: [porter.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join tokenized stem words into a string\n",
    "data['txt_str'] = data.txt_stems.apply(lambda s: ' '.join(map(str, s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_group = data.groupby('cat', as_index=False, group_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balancedDF = cat_group.apply(lambda s: s.sample(25000, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neural Network Classifier with Scikit\n",
    "\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using scikit-learn. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def documents(corpus):\n",
    "    return list(corpus.reviews())\n",
    "\n",
    "def continuous(corpus):\n",
    "    return list(corpus.scores())\n",
    "\n",
    "def make_categorical(corpus):\n",
    "    \"\"\"\n",
    "    terrible : 0.0 < y <= 3.0\n",
    "    okay     : 3.0 < y <= 5.0\n",
    "    great    : 5.0 < y <= 7.0\n",
    "    amazing  : 7.0 < y <= 10.1\n",
    "    \"\"\"\n",
    "    return np.digitize(continuous(corpus), [0.0, 3.0, 5.0, 7.0, 10.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(path, model, continuous=True, saveto=None, cv=12):\n",
    "    \"\"\"\n",
    "    Trains model from corpus at specified path; constructing cross-validation\n",
    "    scores using the cv parameter, then fitting the model on the full data.\n",
    "    Returns the scores.\n",
    "    \"\"\"\n",
    "    # load the corpus data and labels for classification\n",
    "    corpus = PickledReviewsReader(path)\n",
    "    X = documents(corpus)\n",
    "    if continuous:\n",
    "        y = continuous(corpus)\n",
    "        scoring = 'r2_score'\n",
    "    else:\n",
    "        y = make_categorical(corpus)\n",
    "        scoring = 'f1_score'\n",
    "\n",
    "    # compute cross-validation scores\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n",
    "    # write to disk if specified\n",
    "    if saveto:\n",
    "        joblib.dump(model, saveto)\n",
    "\n",
    "    # fit the model on entire dataset\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # return scores\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    from transformer import TextNormalizer\n",
    "    from reader import PickledReviewsReader\n",
    "\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    # path to postpreprocessed, part-of-speech tagged review corpus\n",
    "    cpath = '../review_corpus_proc'\n",
    "\n",
    "    regressor = Pipeline([\n",
    "        ('norm', TextNormalizer()),\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('ann', MLPRegressor(hidden_layer_sizes=[500,150], verbose=True))\n",
    "    ])\n",
    "    regression_scores = train_model(cpath, regressor, continuous=True)\n",
    "\n",
    "    classifier = Pipeline([\n",
    "        ('norm', TextNormalizer()),\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('ann', MLPClassifier(hidden_layer_sizes=[500,150], verbose=True))\n",
    "    ])\n",
    "    classifer_scores = train_model(cpath, classifier, continuous=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network Classifier with Keras\n",
    "\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using Keras. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURES = 5000\n",
    "\n",
    "# start neural network\n",
    "nn = Sequential()\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "nn.add(layers.Dense(units=500,\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(N_FEATURES,)))\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "nn.add(layers.Dense(units=150, activation=\"relu\"))\n",
    "\n",
    "# Add fully connected layer with a softmax activation function\n",
    "nn.add(layers.Dense(units=3, activation=\"softmax\"))\n",
    "\n",
    "# Compile neural network\n",
    "nn.compile(loss=\"categorical_crossentropy\", # Cross-entropy\n",
    "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"]) # Accuracy performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train neural network\n",
    "history = nn.fit(features_train, # features\n",
    "                      target_train, # target\n",
    "                      epochs=3, # three epochs\n",
    "                      verbose=1, \n",
    "                      batch_size=50, # number of observations per batch\n",
    "                      validation_data=(features_test, target_test)) # test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "N_FEATURES = 5000\n",
    "N_CLASSES = 4\n",
    "\n",
    "def build_network():\n",
    "    \"\"\"\n",
    "    Create a function that returns a compiled neural network\n",
    "    \"\"\"\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(500, activation='relu', input_shape=(N_FEATURES,)))\n",
    "    nn.add(Dense(150, activation='relu'))\n",
    "    nn.add(Dense(N_CLASSES, activation='softmax'))\n",
    "    nn.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from transformer import TextNormalizer\n",
    "    from keras.wrappers.scikit_learn import KerasClassifier\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('norm', TextNormalizer()),\n",
    "        ('vect', TfidfVectorizer(max_features=N_FEATURES)),\n",
    "        ('nn', KerasClassifier(build_fn=build_network,\n",
    "                               epochs=200,\n",
    "                               batch_size=128))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(path, model, saveto=None, cv=12):\n",
    "    \"\"\"\n",
    "    Trains model from corpus at specified path and fits on full data.\n",
    "    If a saveto dictionary is specified, writes Keras and Sklearn\n",
    "    pipeline components to disk separately. Returns the scores.\n",
    "    \"\"\"\n",
    "    corpus = PickledReviewsReader(path)\n",
    "    X = documents(corpus)\n",
    "    y = make_categorical(corpus)\n",
    "\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    if saveto:\n",
    "        model.steps[-1][1].model.save(saveto['keras_model'])\n",
    "        model.steps.pop(-1)\n",
    "        joblib.dump(model, saveto['sklearn_pipe'])\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpath = '../review_corpus_proc'\n",
    "mpath = {\n",
    "    'keras_model'  : 'keras_nn.h5',\n",
    "    'sklearn_pipe' : 'pipeline.pkl'\n",
    "}\n",
    "scores = train_model(cpath, pipeline, saveto=mpath, cv=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classifying Images\n",
    "\n",
    "In chapter 20 of the Machine Learning with Python Cookbook, implement the code found in section 20.15 classify MSINT images using a convolutional neural network. Report the accuracy of your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install intel-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set that the color channel value will be first\n",
    "K.set_image_data_format(\"channels_first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "np.random.seed(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set image information\n",
    "channels=1\n",
    "height=28\n",
    "width=28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and target from mnist data\n",
    "(data_train, target_train), (data_test, target_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape training image data into features\n",
    "data_train = data_train.reshape(data_train.shape[0], channels, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape test image data into features\n",
    "data_test = data_test.reshape(data_test.shape[0], channels, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale pixel intensity to between 0 and 1\n",
    "features_train = data_train/255\n",
    "features_test = data_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode target\n",
    "target_train = np_utils.to_categorical(target_train)\n",
    "target_test = np_utils.to_categorical(target_test)\n",
    "numberofclasses = target_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start neural nerwork\n",
    "network = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add convolutional layer with 64 filters, a 5x5 window and ReLU activation function\n",
    "network.add(Conv2D(filters=64,\n",
    "                  kernel_size=(5,5),\n",
    "                  input_shape=(channels, width, height),\n",
    "                  activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add max pooling layer with a 2x2 window\n",
    "network.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dropout layer\n",
    "network.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add layer to flatten input\n",
    "network.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a fully connected layer of 128 units with a ReLU activation function\n",
    "network.add(Dense(128, activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a dropout layer\n",
    "network.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a fully connected layer with a softmax activation funtion\n",
    "network.add(Dense(numberofclasses, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile a neural network\n",
    "network.compile(loss=\"categorical_crossentropy\", #cross-entropy\n",
    "               optimizer=\"rmsprop\", #root mean square propagation\n",
    "               metrics=[\"accuracy\"]) #accuracy performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train neural network\n",
    "network.fit(features_train, # Features\n",
    "            target_train, # Target\n",
    "            epochs=2, # Number of epochs\n",
    "            verbose=0, # Don't print description after each epoch\n",
    "            batch_size=1000, # Number of observations per batch\n",
    "            validation_data=(features_test, target_test)) # Data for evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report accuracy\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "predictions = network.predict(features_test)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "y_test = np.argmax(target_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print precision, recall, F1-score, and accuracy\n",
    "print(classification_report(y_test,predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
